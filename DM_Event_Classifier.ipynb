{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0709b7b1",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Machine Learning for Dark Matter Signal Classification</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1.3em; text-align: center\"><b>Owner:</b> Lucas Rocha Castro</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1em;\"> \n",
    "In the process of searching dor a possible Dark Matter (DM) particle, astrophysical observations are one of the pilars of DM indirect detection. When a target is observed, however, it is well known that noise may interfere with the obtained signal, and it is really important to develop strategies in order to be able to separate what is a potential DM particle signal and what is noisy, not usable data. In this project, I stablished a separation system that is simplified, yet physically relevant, taking into account the characteristics of the received signal, such as: the energy received; the angle at which it was intercepted; the particle's velocity and others.\n",
    "Using this separation, I generated many simulated data points using basic Python functions, and developed a trained algorithm using basic Machine Learning (ML) to evaluate if the machine was able to identify the two type of potential signals or not. \n",
    "It is a simple and yet useful project. As an undergraduate in Physics, my main objective is to be able to study ML and other computer science topics while integrating it with my currently researched topics: Dark Matter, Cosmology and Astrophysics.\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import maxwell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01707b03",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Data simulation</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1em;\"> \n",
    "I will be following a simplified model to identify what really should be Dark Matter signals versus what is background noise. To <b>Dark Matter</b> signal: <br>\n",
    "- $E$ (energy) will be a normal distribution with its peak located at ~50GeV, a reasonable value when dealing with indirect detection, such as Gamma Rays. <br>\n",
    "- $v$ (velocity) will follow a Maxwell-Boltzmann distribution, because according to the Standard Halo Model, the DM particles form an isotropical gas in gravitational equilibrium. Its scale is due to the Sun's velocity around the Galactic Center, which is $v_{\\odot} = 220 km/s$. <br>\n",
    "- $\\theta$, as the angle at which those particles are received. Note that it is not isotropic due to the Sun's orbit around the GC, and that causes a type of \"dark matter wind\", at which there is in fact a prefered angle. <br>\n",
    "- $\\phi$ represents the rotational symmetry due to the Halo's format. <br>\n",
    "- $r$ represents a DM model that decays exponentially. It is indeed not the current model used (such as NFW, Einasto, Moore, etc.) but in this case, to simplify the analysis and not deal with major computational problems, it is a great approximation. <br>\n",
    "<br>\n",
    "In contrast, the majority of <b>background data</b> is distributed on a uniform dataset, which is due to its homogeneity and isotropy, without any peaks because it has no specific events and, furthermore, it is a mix of different processes (radioactivity, cosmic rays, equipment noise and more), which brings a lot of randomness to all parameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9866bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataset \n",
    "def generate_dm_events(N): \n",
    "    E = np.random.normal(loc=50, scale=10, size=N) \n",
    "    v = maxwell.rvs(scale=220, size=N) \n",
    "    theta = np.random.normal(loc=np.pi/2, scale=0.4, size=N) \n",
    "    phi = np.random.uniform(0, 2*np.pi, size=N) \n",
    "    r = np.random.exponential(scale=8, size=N) \n",
    "    \n",
    "    return pd.DataFrame({ \"E\": E, \n",
    "                         \"v\": v, \n",
    "                         \"theta\": theta, \n",
    "                         \"phi\": phi, \n",
    "                         \"r\": r, \n",
    "                         \"label\": 1 })\n",
    "\n",
    "def generate_noise(N):\n",
    "    E = np.random.exponential(scale=40, size=N) \n",
    "    v = np.random.uniform(0, 600, size=N) \n",
    "    theta = np.random.uniform(0, np.pi, size=N) \n",
    "    phi = np.random.uniform(0, 2*np.pi, size=N) \n",
    "    r = np.random.uniform(0, 20, size=N) \n",
    "    \n",
    "    return pd.DataFrame({ \"E\": E, \n",
    "                         \"v\": v, \n",
    "                         \"theta\": theta, \n",
    "                         \"phi\": phi, \n",
    "                         \"r\": r, \n",
    "                         \"label\": 0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d31ae16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E           v     theta       phi          r  label\n",
      "0      54.166063  465.564682  2.056886  0.162574  46.544763      1\n",
      "1     105.983932  295.214998  1.479003  1.165806   4.721523      0\n",
      "2      55.466821  310.029380  0.782318  5.225734   4.604419      1\n",
      "3      34.050060  153.211239  1.307326  5.048844   8.400338      1\n",
      "4      54.710806  452.396770  0.822376  3.225239   1.287275      1\n",
      "...          ...         ...       ...       ...        ...    ...\n",
      "9995   76.494516  289.338491  0.634798  2.982687  14.055532      0\n",
      "9996   29.701707  228.181610  0.948288  5.979220   4.057441      1\n",
      "9997   41.718264  207.594039  2.761381  1.369940   8.272060      0\n",
      "9998   42.230783  285.207607  2.972705  1.717316  18.936708      0\n",
      "9999   51.192400  347.563707  1.090117  5.193784   7.262237      1\n",
      "\n",
      "[10000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "dm = generate_dm_events(N)\n",
    "bg = generate_noise(N)\n",
    "\n",
    "data = pd.concat([dm, bg]).sample(frac=1).reset_index(drop=True) \n",
    "data.to_csv(\"events.csv\", index=False)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d69f37",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Machine Learning</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
