{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b907b",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Machine Learning for Dark Matter Signal Classification</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1.3em; text-align: center\"><b>Owner:</b> Lucas Rocha Castro</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1em;\"> \n",
    "In the process of searching dor a possible Dark Matter (DM) particle, astrophysical observations are one of the pilars of DM indirect detection. When a target is observed, however, it is well known that noise may interfere with the obtained signal, and it is really important to develop strategies in order to be able to separate what is a potential DM particle signal and what is noisy, not usable data. In this project, I stablished a separation system that is simplified, yet physically relevant, taking into account the characteristics of the received signal, such as: the energy received; the angle at which it was intercepted; the particle's velocity and others.\n",
    "Using this separation, I generated many simulated data points using basic Python functions, and developed a trained algorithm using basic Machine Learning (ML) to evaluate if the machine was able to identify the two type of potential signals or not. \n",
    "It is a simple and yet useful project. As an undergraduate in Physics, my main objective is to be able to study ML and other computer science topics while integrating it with my currently researched topics: Dark Matter, Cosmology and Astrophysics.\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11b087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import maxwell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf67bac",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Data simulation</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1em;\"> \n",
    "I will be following a simplified model to identify what really should be Dark Matter signals versus what is background noise. <br>\n",
    "    <br>\n",
    "Regarding <b>Dark Matter</b> signal: <br>\n",
    "- $E$ (energy) will be a normal distribution with its peak located at ~50GeV, a reasonable value when dealing with indirect detection, such as Gamma Rays. <br>\n",
    "- $v$ (velocity) will follow a Maxwell-Boltzmann distribution, because according to the Standard Halo Model, the DM particles form an isotropical gas in gravitational equilibrium. Its scale is due to the Sun's velocity around the Galactic Center, which is $v_{\\odot} = 220 km/s$. <br>\n",
    "- $\\theta$, as the angle at which those particles are received. Note that it is not isotropic due to the Sun's orbit around the GC, and that causes a type of \"dark matter wind\", at which there is in fact a prefered angle. <br>\n",
    "- $\\phi$ represents the rotational symmetry due to the Halo's format. <br>\n",
    "- $r$ represents a DM model that decays exponentially. It is indeed not the current model used (such as NFW, Einasto, Moore, etc.) but in this case, to simplify the analysis and not deal with major computational problems, it is a great approximation. <br>\n",
    "<br>\n",
    "In contrast, the majority of <b>background data</b> is distributed on a uniform dataset, which is due to its homogeneity and isotropy, without any peaks because it has no specific events and, furthermore, it is a mix of different processes (radioactivity, cosmic rays, equipment noise and more), which brings a lot of randomness to all parameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7eeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataset \n",
    "def generate_dm_events(N): \n",
    "    E = np.random.normal(loc=50, scale=10, size=N) \n",
    "    v = maxwell.rvs(scale=220, size=N) \n",
    "    theta = np.random.normal(loc=np.pi/2, scale=0.4, size=N) \n",
    "    phi = np.random.uniform(0, 2*np.pi, size=N) \n",
    "    r = np.random.exponential(scale=8, size=N) \n",
    "    \n",
    "    return pd.DataFrame({ \"E\": E, \n",
    "                         \"v\": v, \n",
    "                         \"theta\": theta, \n",
    "                         \"phi\": phi, \n",
    "                         \"r\": r, \n",
    "                         \"label\": 1 })\n",
    "\n",
    "def generate_noise(N):\n",
    "    E = np.random.exponential(scale=40, size=N) \n",
    "    v = np.random.uniform(0, 600, size=N) \n",
    "    theta = np.random.uniform(0, np.pi, size=N) \n",
    "    phi = np.random.uniform(0, 2*np.pi, size=N) \n",
    "    r = np.random.uniform(0, 20, size=N) \n",
    "    \n",
    "    return pd.DataFrame({ \"E\": E, \n",
    "                         \"v\": v, \n",
    "                         \"theta\": theta, \n",
    "                         \"phi\": phi, \n",
    "                         \"r\": r, \n",
    "                         \"label\": 0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3e5ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               E           v     theta       phi          r  label\n",
      "0      61.025803  310.879390  0.817846  5.422012   0.287951      1\n",
      "1      40.934842  479.906652  2.215813  3.034172  18.788725      0\n",
      "2      28.563479  117.470710  1.151123  2.493208   1.247649      1\n",
      "3      51.732675  310.756166  1.078034  4.802839   3.076940      1\n",
      "4       6.636092  356.295642  0.366266  0.994590  15.430433      0\n",
      "...          ...         ...       ...       ...        ...    ...\n",
      "9995   44.422658  336.451925  1.243652  2.851176   0.645467      1\n",
      "9996   42.221867  584.632380  1.348136  5.360222   5.895090      1\n",
      "9997  100.119918  356.295004  2.314896  4.209037  10.551552      0\n",
      "9998   46.495388   72.528357  2.584907  1.152684  12.228210      0\n",
      "9999   47.412793  499.894128  1.462030  0.709926   1.186067      1\n",
      "\n",
      "[10000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "dm = generate_dm_events(N)\n",
    "bg = generate_noise(N)\n",
    "\n",
    "data = pd.concat([dm, bg]).sample(frac=1).reset_index(drop=True) \n",
    "data.to_csv(\"events.csv\", index=False)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e5751",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Machine Learning</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0188dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43a11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataset\n",
    "blind_data = data.drop('label', axis = 1) #Separating labels from the data\n",
    "answer = data[\"label\"] #Answers: 0 for background, 1 for DM\n",
    "\n",
    "#Separating train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(blind_data, \n",
    "                                                    answer, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c31df",
   "metadata": {},
   "source": [
    "#### <p style=\"font-family:cm; font-size:1.4em;\">Logistic Regression</p>\n",
    "<p style=\"font-family:cm; font-size:1.1em;\"> \n",
    "Logistic Regression is a ML model that is only applied to linear data, in which every parameter is associated to a coefficient. It then plots a hyperplane in the 4d space formed by the 4 parameters in question. If a data point falls into one side of the hyperplane, it will be considered Dark Matter. If not, then it will be counted as background noise. <br>\n",
    "<br>\n",
    "At the end, the results are compared to the original labels and the accuracy is computed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae06a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaler test accuracy: 63.90%\n"
     ]
    }
   ],
   "source": [
    "#Putting all into a same scale to avoid absolute value bias\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "#Applying Logistic Regression to the standardized dataset\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Computing the final accuracy of DM-signal separation\n",
    "accuracy_LR = model_LR.score(X_test_scaled, y_test)\n",
    "print(f\"Standard Scaler test accuracy: {accuracy_LR * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b335c",
   "metadata": {},
   "source": [
    "#### <p style=\"font-family:cm; font-size:1.4em;\">Random Forest</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1.1em;\">\n",
    "This is a supervised ML tool that is going to be used for classification purposes by creating various decision trees until the number of estimators is reached. The same 80/20 dataset will be used here for training and testing.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26acc687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest test accuracy: 88.80%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Activating Random Forest model to my dataset\n",
    "model_RF = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "model_RF.fit(X_train, y_train)\n",
    "\n",
    "#Test and check for the accuracy\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "accuracy_RF = accuracy_score(y_test, y_pred_RF)\n",
    "\n",
    "print(f\"Random Forest test accuracy: {accuracy_RF * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f45c4",
   "metadata": {},
   "source": [
    "#### <p style=\"font-family:cm; font-size:1.4em;\">SVM (Support Vector Machines)</p>\n",
    "\n",
    "<p style=\"font-family:cm; font-size:1.1em;\">\n",
    "SVM is a supervised ML tool that analyses data and separates the two different groups by finding a the best type of frontier that sets them apart, and that's why it can be used to linear and non-linear data (differently from Logistic Regression). It then spits out its results, which then are compared to the original ones (data labels). \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0374c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test accuracy: 88.90%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_SVM = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "\n",
    "#Again, training ML model with the same dataset\n",
    "model_SVM.fit(X_train_scaled, y_train)\n",
    "y_predict_SVM = model_SVM.predict(X_test_scaled)\n",
    "\n",
    "#Testing and accuracy check\n",
    "accuracy_SVM = accuracy_score(y_test, y_predict_SVM)\n",
    "print(f\"SVM test accuracy: {accuracy_SVM * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0fe06",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; font-family:cm; font-size:1.8em;\">Conclusion</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760626f",
   "metadata": {},
   "source": [
    "<p style=\"font-family:cm; font-size:1em;\">\n",
    "The three tested models showed clearly distinct behaviors in the task of classifying dark matter events versus background. Logistic regression achieved significantly lower performance, indicating that the separation between signal and background cannot be described by a simple linear decision boundary in the space of physical observables. This suggests that the discriminating information is encoded in correlations among variables rather than in a single dominant feature. Both the Support Vector Machine and the Random Forest reached very similar performance levels, with accuracies close to 90%, indicating a well-defined and robust non-linear separation in the synthetic dataset. The slight advantage of the Random Forest, combined with its interpretability through feature importance analysis, makes it particularly suitable for physical interpretation, while the SVM stands out as an effective classification tool with limited direct physical insight.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
